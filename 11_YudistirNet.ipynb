{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training RNN using World's data\n",
    "\n",
    "This is based on `COVID-19 growth prediction using multivariate\n",
    "long short term memory` by `Novanto Yudistira`\n",
    "\n",
    "https://arxiv.org/pdf/2005.04809.pdf\n",
    "\n",
    "https://github.com/VICS-CORE/lstmcorona/blob/master/lstm.py\n",
    "\n",
    "- We've aligned all countries' inputs rather than taking an absolute timeline. We start when number of confirmed cases in the country has crossed 100.\n",
    "- We've normalised data by dividing by a population factor. That way the network can learn what factor of population will be affected.\n",
    "- Rather than using the entire timeline as an input as suggested by NYudistira, we're training a fixed window (e.g. 20 days) so that the model learns to predict the future by looking at present data. The problem with fixed window approach is that some countries have peaked, while others have not. Also few countries start early, and some start late.\n",
    "\n",
    "#### Ideas\n",
    "- One idea is to train a network to predict SIR buckets\n",
    "- Another is to train only with most populous countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests as rq\n",
    "import datetime as dt\n",
    "import torch\n",
    "\n",
    "tnn = torch.nn\n",
    "top = torch.optim\n",
    "from torch.utils import data as tdt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA=\"cuda:0\"\n",
    "CPU=\"cpu\"\n",
    "device = torch.device(CUDA if torch.cuda.is_available() else CPU)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read OWID data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n1 csv/owid-covid-data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['location', 'date', 'total_cases', 'new_cases', 'total_deaths', 'new_deaths', 'population']\n",
    "dates = ['date']\n",
    "df = pd.read_csv(\"csv/owid-covid-data.csv\", \n",
    "                 usecols=cols,\n",
    "                 parse_dates=dates)\n",
    "df.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IP_SEQ_LEN = 20\n",
    "OP_SEQ_LEN = 10\n",
    "VAL_RATIO = 0.3\n",
    "\n",
    "ip_trn = []\n",
    "op_trn = []\n",
    "\n",
    "countries = df['location'].unique()\n",
    "c = 0\n",
    "for country in countries:\n",
    "    if country in ['World', 'International']: # Countries to be skipped\n",
    "        continue\n",
    "    country_df = df.loc[df.location == country]\n",
    "    tot_cases_gt_100 = (country_df['total_cases'] >= 100)\n",
    "    country_df = country_df.loc[tot_cases_gt_100]\n",
    "    \n",
    "    if len(country_df) >= IP_SEQ_LEN + OP_SEQ_LEN:\n",
    "        c += 1\n",
    "        pop = country_df['population'].iloc[0]\n",
    "        print(c, country, len(country_df), pop)\n",
    "        daily_cases = np.array(country_df['new_cases'].rolling(7, center=True, min_periods=1).mean() * 1000 / pop, dtype=np.float32)\n",
    "\n",
    "        if country in ['India']: # Countries to be tested. Not included in training data.\n",
    "            continue\n",
    "\n",
    "        for i in range(len(country_df) - IP_SEQ_LEN - OP_SEQ_LEN + 1):\n",
    "            ip_trn.append(daily_cases[i : i+IP_SEQ_LEN])\n",
    "            op_trn.append(daily_cases[i+IP_SEQ_LEN : i+IP_SEQ_LEN+OP_SEQ_LEN])\n",
    "\n",
    "ip_trn = torch.from_numpy(np.array(ip_trn, dtype=np.float32))\n",
    "op_trn = torch.from_numpy(np.array(op_trn, dtype=np.float32))\n",
    "dataset = tdt.TensorDataset(ip_trn, op_trn)\n",
    "\n",
    "val_len = int(VAL_RATIO * len(dataset))\n",
    "trn_len = len(dataset) - val_len\n",
    "trn_set, val_set = tdt.random_split(dataset, (trn_len, val_len))\n",
    "print(\"Training data:\", trn_len, \"Validation data:\", val_len)\n",
    "\n",
    "trn_loader = tdt.DataLoader(trn_set, shuffle=True, batch_size=1)\n",
    "val_loader = tdt.DataLoader(val_set, shuffle=True, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YudistirNet(tnn.Module):\n",
    "    def __init__(self, ip_seq_len=1, op_seq_len=1, hidden_size=1, num_layers=1):\n",
    "        super(YudistirNet, self).__init__()\n",
    "        \n",
    "        self.ip_seq_len = ip_seq_len\n",
    "        self.op_seq_len = op_seq_len\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.lstm = tnn.LSTM(input_size=1, hidden_size=self.hidden_size, num_layers=self.num_layers)\n",
    "        self.linear = tnn.Linear(self.hidden_size * self.ip_seq_len, self.op_seq_len)\n",
    "        self.sigmoid = tnn.Sigmoid()\n",
    "    \n",
    "    def forward(self, ip):\n",
    "        lstm_out, _ = self.lstm(ip)\n",
    "        linear_out = self.linear(lstm_out.view(self.hidden_size * self.ip_seq_len))\n",
    "        sigmoid_out = self.sigmoid(linear_out.view(self.op_seq_len))\n",
    "        return sigmoid_out\n",
    "    \n",
    "    def predict(self, ip):\n",
    "        with torch.no_grad():\n",
    "            preds = self.forward(ip)\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(epoch, model, optimizer, trn_losses, val_losses):\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'trn_losses': trn_losses,\n",
    "        'val_losses': val_losses\n",
    "    }, \"latest.pt\")\n",
    "    print(\"Checkpoint saved\")\n",
    "    \n",
    "def load_checkpoint():\n",
    "    cp = torch.load(\"latest.pt\")\n",
    "    print(\"Checkpoint loaded\")\n",
    "    return cp['epoch'], cp['model_state_dict'], cp['optimizer_state_dict'], cp['trn_losses'], cp['val_losses']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 1\n",
    "NUM_LAYERS = 1\n",
    "LEARNING_RATE = 0.01\n",
    "NUM_EPOCHS = 31\n",
    "\n",
    "model = YudistirNet(ip_seq_len=IP_SEQ_LEN, op_seq_len=OP_SEQ_LEN, hidden_size=HIDDEN_SIZE, num_layers=NUM_LAYERS)\n",
    "model = model.to(device)\n",
    "\n",
    "loss_fn = tnn.MSELoss()\n",
    "optimizer = top.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "trn_loss_vals = []\n",
    "val_loss_vals = []\n",
    "e = 0\n",
    "\n",
    "resume = False\n",
    "if resume:\n",
    "    e, model_dict, optimizer_dict, trn_loss_vals, val_loss_vals = load_checkpoint()\n",
    "    e+=1\n",
    "    model.load_state_dict(model_dict)\n",
    "    optimizer.load_state_dict(optimizer_dict)\n",
    "\n",
    "# TRAIN\n",
    "print(\"BEGIN: [\", dt.datetime.now(), \"]\")\n",
    "while e < NUM_EPOCHS:\n",
    "    model.train()\n",
    "    trn_losses = []\n",
    "    for data in trn_loader:\n",
    "        ip, op = data\n",
    "        ip = ip.to(device)\n",
    "        op = op.to(device)\n",
    "        optimizer.zero_grad() # set grads to 0\n",
    "        preds = model(ip.view(IP_SEQ_LEN, 1, 1)) # predict\n",
    "        loss = loss_fn(preds, op.view(OP_SEQ_LEN)) # calc loss\n",
    "        loss.backward() # calc and assign grads\n",
    "        optimizer.step() # update weights\n",
    "        trn_losses.append(loss) # logging\n",
    "    avg_trn_loss = torch.stack(trn_losses).mean().item() * 10000\n",
    "    trn_loss_vals.append(avg_trn_loss)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_losses = []\n",
    "        for data in val_loader:\n",
    "            ip, op = data\n",
    "            ip = ip.to(device)\n",
    "            op = op.to(device)\n",
    "            preds = model(ip.view(IP_SEQ_LEN, 1, 1))\n",
    "            loss = loss_fn(preds, op.view(OP_SEQ_LEN))\n",
    "            val_losses.append(loss)\n",
    "        avg_val_loss = torch.stack(val_losses).mean().item() * 10000\n",
    "        val_loss_vals.append(avg_val_loss)\n",
    "    \n",
    "    if e%10==0:\n",
    "        print(\"[\", dt.datetime.now(), \"] epoch:\", f\"{e:3}\", \"avg_val_loss:\", f\"{avg_val_loss: .5f}\", \"avg_trn_loss:\", f\"{avg_trn_loss: .5f}\")\n",
    "        save_checkpoint(e, model, optimizer, trn_loss_vals, val_loss_vals)\n",
    "    e+=1\n",
    "\n",
    "print(\"END: [\", dt.datetime.now(), \"]\")\n",
    "\n",
    "df_trn_loss = pd.DataFrame({\n",
    "    'trn_loss': trn_loss_vals,\n",
    "    'val_loss': val_loss_vals\n",
    "})\n",
    "_ = df_trn_loss.plot(\n",
    "    y=['trn_loss', 'val_loss'],\n",
    "    title=['Training loss per epoch', 'Validation loss per epoch'],\n",
    "    subplots=True,\n",
    "    figsize=(5,8),\n",
    "    sharex=False,\n",
    "    logy=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 1\n",
    "NUM_LAYERS = 1\n",
    "_, md, _, _, _ = load_checkpoint()\n",
    "model = YudistirNet(ip_seq_len=IP_SEQ_LEN, op_seq_len=OP_SEQ_LEN, hidden_size=HIDDEN_SIZE, num_layers=NUM_LAYERS)\n",
    "model = model.to(device)\n",
    "model.load_state_dict(md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evalute fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = \"Brazil\"\n",
    "pop_fct = df.loc[df.location==c, 'population'].iloc[0] / 1000\n",
    "\n",
    "all_preds = []\n",
    "pred_vals = []\n",
    "out_vals = []\n",
    "\n",
    "test_data = np.array(df.loc[(df.location==c) & (df.total_cases>=100), 'new_cases'].rolling(7, center=True, min_periods=1).mean() / pop_fct, dtype=np.float32)\n",
    "\n",
    "for i in range(len(test_data) - IP_SEQ_LEN - OP_SEQ_LEN + 1):\n",
    "    ip = torch.tensor(test_data[i : i+IP_SEQ_LEN])\n",
    "    op = torch.tensor(test_data[i+IP_SEQ_LEN : i+IP_SEQ_LEN+OP_SEQ_LEN])\n",
    "    ip = ip.to(device)\n",
    "    op = op.to(device)\n",
    "\n",
    "    pred = model.predict(ip.view(IP_SEQ_LEN, 1, 1))    \n",
    "#     if i==0: # prepend first input\n",
    "#         pred_vals.extend(ip.view(IP_SEQ_LEN).numpy() * pop_fct)\n",
    "#         out_vals.extend(ip.view(IP_SEQ_LEN).numpy() * pop_fct)        \n",
    "    all_preds.append(pred.view(OP_SEQ_LEN).cpu().numpy() * pop_fct)\n",
    "    pred_vals.append(pred.view(OP_SEQ_LEN).cpu().numpy()[0] * pop_fct)\n",
    "    out_vals.append(op.view(OP_SEQ_LEN).cpu().numpy()[0] * pop_fct)\n",
    "\n",
    "# last N-1 values\n",
    "out_vals.extend(op.view(OP_SEQ_LEN).cpu().numpy()[1:] * pop_fct)\n",
    "pred_vals.extend(([np.NaN] * OP_SEQ_LEN)[1:]) # pad with NaN\n",
    "\n",
    "cmp_df = pd.DataFrame({\n",
    "    'actual': out_vals,\n",
    "    'predicted0': pred_vals\n",
    "})\n",
    "ax = cmp_df.plot(\n",
    "    figsize=(20,8),\n",
    "    lw=3,\n",
    "    title=c\n",
    ")\n",
    "\n",
    "# plot predictions\n",
    "i=0\n",
    "for pred in all_preds:\n",
    "    cmp_df['predicted_cases'] = np.NaN\n",
    "    cmp_df.loc[i:i+OP_SEQ_LEN-1, 'predicted_cases'] = pred\n",
    "    cmp_df.plot(y='predicted_cases', ax=ax, legend=False)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test (predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = \"Brazil\"\n",
    "pop_fct = df.loc[df.location==c, 'population'].iloc[0] / 1000\n",
    "test_data = np.array(df.loc[(df.location==c) & (df.total_cases>=100), 'new_cases'].rolling(7, center=True, min_periods=1).mean() / pop_fct, dtype=np.float32)\n",
    "\n",
    "ip = torch.tensor(\n",
    "    test_data[-IP_SEQ_LEN:],\n",
    "    dtype=torch.float32\n",
    ")\n",
    "ip = ip.to(device)\n",
    "pred = model.predict(ip.view(IP_SEQ_LEN, 1, 1))\n",
    "orig_df = pd.DataFrame({\n",
    "    'actual': test_data * pop_fct\n",
    "})\n",
    "fut_df = pd.DataFrame({\n",
    "    'predicted': pred.cpu().numpy() * pop_fct\n",
    "})\n",
    "orig_df = orig_df.append(fut_df, ignore_index=True, sort=False)\n",
    "_ = orig_df.plot(title=c)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
