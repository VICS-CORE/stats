{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training RNN using World's data\n",
    "\n",
    "This is based on `COVID-19 growth prediction using multivariate\n",
    "long short term memory` by `Novanto Yudistira`\n",
    "\n",
    "https://arxiv.org/pdf/2005.04809.pdf\n",
    "\n",
    "https://github.com/VICS-CORE/lstmcorona/blob/master/lstm.py\n",
    "\n",
    "- We've aligned all countries' inputs rather than taking an absolute timeline. We start when cumulative number of confirmed cases in the country has crossed 100.\n",
    "- We've normalised data by dividing by a population factor. That way the network can learn a general understanding of the pattern irrespective of the country's population.\n",
    "- Rather than using the entire timeline as an input as suggested by NYudistira, we're training a fixed window (e.g. 20 days) so that the model learns to predict the future by looking at present data. The problem with fixed window approach is that some countries have peaked, while others have not. Also few countries start early, and some start late.\n",
    "- The paper uses a multivariate network with confirmed, recovered and deceased data. However this'd increase computation time and hence we're restricting ourselves to a univariate model with confirmed cases as the only parameter.\n",
    "\n",
    "#### Other ideas\n",
    "- One idea is to train the current net with only the most populous countries' data, since their behaviour would be similar to India's.\n",
    "- Adding metrics like humidity, population density, lockdown intensity etc might be beneficial and should have some correlation with the growth in cases. But this'd need more computation power.\n",
    "- Another idea is to train a neuralnet to predict SIR like buckets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests as rq\n",
    "import datetime as dt\n",
    "import torch\n",
    "import json\n",
    "\n",
    "tnn = torch.nn\n",
    "top = torch.optim\n",
    "from torch.utils import data as tdt\n",
    "\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "from matplotlib.dates import DayLocator, AutoDateLocator, ConciseDateFormatter\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA=\"cuda:0\"\n",
    "CPU=\"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(CUDA)\n",
    "    cd = torch.cuda.current_device()\n",
    "    print(\"Num devices:\", torch.cuda.device_count())\n",
    "    print(\"Current device:\", cd)\n",
    "    print(\"Device name:\", torch.cuda.get_device_name(cd))\n",
    "    print(\"Device props:\", torch.cuda.get_device_properties(cd))\n",
    "    print(torch.cuda.memory_summary(cd))\n",
    "else:\n",
    "    device = torch.device(CPU)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define paths\n",
    "DATA_DIR = 'data'\n",
    "MODELS_DIR = 'models'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Colab only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd 'drive/My Drive/CS/colab/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat /proc/cpuinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat /proc/meminfo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read OWID data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl https://covid.ourworldindata.org/data/owid-covid-data.csv --output data/owid-covid-data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n1 data/owid-covid-data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['location', 'date', 'total_cases', 'new_cases', 'total_deaths', 'new_deaths', 'population']\n",
    "dates = ['date']\n",
    "df = pd.read_csv(DATA_DIR + \"/owid-covid-data.csv\", \n",
    "                 usecols=cols,\n",
    "                 parse_dates=dates)\n",
    "df.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YudistirNet(tnn.Module):\n",
    "    def __init__(self, ip_seq_len=1, op_seq_len=1, hidden_size=1, num_layers=1):\n",
    "        super(YudistirNet, self).__init__()\n",
    "        \n",
    "        self.ip_seq_len = ip_seq_len\n",
    "        self.op_seq_len = op_seq_len\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.lstm = tnn.LSTM(input_size=1, hidden_size=self.hidden_size, num_layers=self.num_layers, batch_first=True)\n",
    "        self.linear = tnn.Linear(self.hidden_size * self.ip_seq_len, self.op_seq_len)\n",
    "        self.sigmoid = tnn.Sigmoid()\n",
    "    \n",
    "    def forward(self, ip):\n",
    "        lstm_out, _ = self.lstm(ip)\n",
    "        linear_out = self.linear(lstm_out.reshape(-1, self.hidden_size * self.ip_seq_len))\n",
    "        sigmoid_out = self.sigmoid(linear_out.view(-1, self.op_seq_len))\n",
    "        return sigmoid_out\n",
    "    \n",
    "    def predict(self, ip):\n",
    "        with torch.no_grad():\n",
    "            preds = self.forward(ip)\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(epoch, model, optimizer, trn_losses, val_losses, min_val_loss, path=\"\"):\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'trn_losses': trn_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'min_val_loss': min_val_loss\n",
    "    }, path or MODELS_DIR + \"/latest.pt\")\n",
    "    print(\"Checkpoint saved\")\n",
    "    \n",
    "def load_checkpoint(path=\"\", device=\"cpu\"):\n",
    "    cp = torch.load(path or MODELS_DIR + \"/latest.pt\", map_location=device)\n",
    "    print(\"Checkpoint loaded\")\n",
    "    return cp['epoch'], cp['model_state_dict'], cp['optimizer_state_dict'], cp['trn_losses'], cp['val_losses'], cp.get('min_val_loss', np.Inf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "IP_SEQ_LEN = 20\n",
    "OP_SEQ_LEN = 10\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "VAL_RATIO = 0.3\n",
    "\n",
    "HIDDEN_SIZE = 10\n",
    "NUM_LAYERS = 4\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCHS = 3001\n",
    "\n",
    "# to continue training on another model, set resume to true\n",
    "RESUME = False\n",
    "\n",
    "model = YudistirNet(ip_seq_len=IP_SEQ_LEN, op_seq_len=OP_SEQ_LEN, hidden_size=HIDDEN_SIZE, num_layers=NUM_LAYERS)\n",
    "model = model.to(device)\n",
    "\n",
    "loss_fn = tnn.MSELoss()\n",
    "optimizer = top.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_dataset():\n",
    "    ip_trn = []\n",
    "    op_trn = []\n",
    "\n",
    "    countries = df['location'].unique()\n",
    "    pop_countries = ['China', 'United States', 'Indonesia', 'Pakistan', 'Brazil', 'Bangladesh', 'Russia', 'Mexico']\n",
    "\n",
    "    c = 0\n",
    "    for country in countries:\n",
    "        if country in ['World', 'International', 'India']: # Countries to be skipped\n",
    "            continue\n",
    "        country_df = df.loc[df.location == country]\n",
    "        tot_cases_gt_100 = (country_df['total_cases'] >= 100)\n",
    "        country_df = country_df.loc[tot_cases_gt_100]\n",
    "\n",
    "        if len(country_df) >= IP_SEQ_LEN + OP_SEQ_LEN:\n",
    "            c += 1\n",
    "            pop = country_df['population'].iloc[0]\n",
    "            print(c, country, len(country_df), pop)\n",
    "            daily_cases = np.array(country_df['new_cases'].rolling(7, center=True, min_periods=1).mean() * 1000 / pop, dtype=np.float32)\n",
    "\n",
    "            for i in range(len(country_df) - IP_SEQ_LEN - OP_SEQ_LEN + 1):\n",
    "                ip_trn.append(daily_cases[i : i+IP_SEQ_LEN])\n",
    "                op_trn.append(daily_cases[i+IP_SEQ_LEN : i+IP_SEQ_LEN+OP_SEQ_LEN])\n",
    "\n",
    "    ip_trn = torch.from_numpy(np.array(ip_trn, dtype=np.float32))\n",
    "    op_trn = torch.from_numpy(np.array(op_trn, dtype=np.float32))\n",
    "    dataset = tdt.TensorDataset(ip_trn, op_trn)\n",
    "\n",
    "    val_len = int(VAL_RATIO * len(dataset))\n",
    "    trn_len = len(dataset) - val_len\n",
    "    trn_set, val_set = tdt.random_split(dataset, (trn_len, val_len))\n",
    "    return trn_set, val_set\n",
    "\n",
    "try:\n",
    "    ds = torch.load(DATA_DIR + '/ds.pt')\n",
    "    trn_set, val_set = ds['trn'], ds['val']\n",
    "    print(\"Loaded dataset from ds.pt\")\n",
    "except FileNotFoundError:\n",
    "    trn_set, val_set = gen_dataset()\n",
    "    torch.save({'trn': trn_set, 'val': val_set}, DATA_DIR + '/ds.pt')\n",
    "    print(\"Saved dataset to ds.pt\")\n",
    "finally:\n",
    "    print(\"Training data:\", len(trn_set), \"Validation data:\", len(val_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_loader = tdt.DataLoader(trn_set, shuffle=True, batch_size=BATCH_SIZE)\n",
    "val_loader = tdt.DataLoader(val_set, shuffle=True, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_loss_vals = []\n",
    "val_loss_vals = []\n",
    "e = 0\n",
    "min_val_loss = np.Inf\n",
    "\n",
    "if RESUME:\n",
    "    e, model_dict, optimizer_dict, trn_loss_vals, val_loss_vals, min_val_loss = load_checkpoint(device=device)\n",
    "    e+=1\n",
    "    model.load_state_dict(model_dict)\n",
    "    optimizer.load_state_dict(optimizer_dict)\n",
    "\n",
    "# TRAIN\n",
    "print(\"BEGIN: [\", dt.datetime.now(), \"]\")\n",
    "while e < NUM_EPOCHS:\n",
    "    model.train()\n",
    "    trn_losses = []\n",
    "    for data in trn_loader:\n",
    "        ip, op = data\n",
    "        ip = ip.to(device)\n",
    "        op = op.to(device)\n",
    "        optimizer.zero_grad() # set grads to 0\n",
    "        preds = model(ip.view(-1, IP_SEQ_LEN, 1)) # predict\n",
    "        loss = loss_fn(preds, op.view(-1, OP_SEQ_LEN)) # calc loss\n",
    "        loss.backward() # calc and assign grads\n",
    "        optimizer.step() # update weights\n",
    "        trn_losses.append(loss) # logging\n",
    "    avg_trn_loss = torch.stack(trn_losses).mean().item() * 10000\n",
    "    trn_loss_vals.append(avg_trn_loss)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_losses = []\n",
    "        for data in val_loader:\n",
    "            ip, op = data\n",
    "            ip = ip.to(device)\n",
    "            op = op.to(device)\n",
    "            preds = model(ip.view(-1, IP_SEQ_LEN, 1))\n",
    "            loss = loss_fn(preds, op.view(-1, OP_SEQ_LEN))\n",
    "            val_losses.append(loss)\n",
    "        avg_val_loss = torch.stack(val_losses).mean().item() * 10000\n",
    "        val_loss_vals.append(avg_val_loss)\n",
    "    \n",
    "    if e%10==0:\n",
    "        print(\"[\", dt.datetime.now(), \"] epoch:\", f\"{e:3}\", \"avg_val_loss:\", f\"{avg_val_loss: .5f}\", \"avg_trn_loss:\", f\"{avg_trn_loss: .5f}\")\n",
    "        if e%100==0:\n",
    "            save_checkpoint(e, model, optimizer, trn_loss_vals, val_loss_vals, min_val_loss, MODELS_DIR + \"/latest-e\" + str(e) + \".pt\")\n",
    "        if avg_val_loss <= min_val_loss:\n",
    "            min_val_loss = avg_val_loss\n",
    "            save_checkpoint(e, model, optimizer, trn_loss_vals, val_loss_vals, min_val_loss, MODELS_DIR + \"/best-e\" + str(e) + \".pt\")\n",
    "    e+=1\n",
    "print(\"END: [\", dt.datetime.now(), \"]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load saved model for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = MODELS_DIR + \"/IP20_OP10_H10_L4_E2001_LR001.pt\"\n",
    "# model_path = \"/home/mayank/Downloads/ds4020-e17xx.pt\"#ds4020-0612-e50x.pt\"\n",
    "e, md, _, trn_loss_vals, val_loss_vals, _ = load_checkpoint(model_path, device=device)\n",
    "print(e)\n",
    "model.load_state_dict(md)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loss = pd.DataFrame({\n",
    "    'trn_loss': trn_loss_vals,\n",
    "    'val_loss': val_loss_vals\n",
    "})\n",
    "df_loss['trn_loss'] = df_loss['trn_loss'].rolling(10).mean()\n",
    "df_loss['val_loss'] = df_loss['val_loss'].rolling(10).mean()\n",
    "_ = df_loss.plot(\n",
    "    y=['trn_loss', 'val_loss'],\n",
    "    title='Loss per epoch',\n",
    "    subplots=True,\n",
    "    figsize=(5,6),\n",
    "    sharex=False,\n",
    "    logy=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evalute fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = \"India\"\n",
    "pop_fct = df.loc[df.location==c, 'population'].iloc[0] / 1000\n",
    "\n",
    "all_preds = []\n",
    "pred_vals = []\n",
    "out_vals = []\n",
    "\n",
    "test_data = np.array(df.loc[(df.location==c) & (df.total_cases>=100), 'new_cases'].rolling(7, center=True, min_periods=1).mean() / pop_fct, dtype=np.float32)\n",
    "\n",
    "for i in range(len(test_data) - IP_SEQ_LEN - OP_SEQ_LEN + 1):\n",
    "    ip = torch.tensor(test_data[i : i+IP_SEQ_LEN])\n",
    "    op = torch.tensor(test_data[i+IP_SEQ_LEN : i+IP_SEQ_LEN+OP_SEQ_LEN])\n",
    "    ip = ip.to(device)\n",
    "    op = op.to(device)\n",
    "\n",
    "    pred = model.predict(ip.view(1, IP_SEQ_LEN, 1))    \n",
    "    if i==0: # prepend first input\n",
    "        out_vals.extend(ip.view(IP_SEQ_LEN).cpu().numpy() * pop_fct)\n",
    "        pred_vals.extend([np.NaN] * IP_SEQ_LEN)\n",
    "    all_preds.append(pred.view(OP_SEQ_LEN).cpu().numpy() * pop_fct)\n",
    "    pred_vals.append(pred.view(OP_SEQ_LEN).cpu().numpy()[0] * pop_fct)\n",
    "    out_vals.append(op.view(OP_SEQ_LEN).cpu().numpy()[0] * pop_fct)\n",
    "\n",
    "# last N-1 values\n",
    "out_vals.extend(op.view(OP_SEQ_LEN).cpu().numpy()[1:] * pop_fct)\n",
    "pred_vals.extend(([np.NaN] * OP_SEQ_LEN)[1:]) # pad with NaN\n",
    "\n",
    "cmp_df = pd.DataFrame({\n",
    "    'actual': out_vals,\n",
    "    'predicted0': pred_vals\n",
    "})\n",
    "\n",
    "# set date\n",
    "start_date = df.loc[(df.location==c) & (df.total_cases>=100)]['date'].iloc[0]\n",
    "end_date = start_date + dt.timedelta(days=cmp_df.index[-1])\n",
    "cmp_df['Date'] = pd.Series([start_date + dt.timedelta(days=i) for i in range(len(cmp_df))])\n",
    "\n",
    "# plot noodles\n",
    "ax=None\n",
    "i=IP_SEQ_LEN\n",
    "mape=[]\n",
    "for pred in all_preds:\n",
    "    cmp_df['predicted_cases'] = np.NaN\n",
    "    cmp_df.loc[i:i+OP_SEQ_LEN-1, 'predicted_cases'] = pred\n",
    "    ax = cmp_df.plot(x='Date', y='predicted_cases', ax=ax, legend=False)\n",
    "    ape = 100 * ((cmp_df['actual'] - cmp_df['predicted_cases']).abs() / cmp_df['actual'])\n",
    "    mape.append(ape.mean())\n",
    "    i+=1\n",
    "acc = f\"{100 - sum(mape)/len(mape):0.2f}%\"\n",
    "\n",
    "# plot primary lines\n",
    "ax = cmp_df.plot(\n",
    "    x='Date',\n",
    "    y=['actual', 'predicted0'],\n",
    "    figsize=(20,8),\n",
    "    lw=5,\n",
    "    title=c + ' | Daily predictions | ' + acc,\n",
    "    ax=ax\n",
    ")\n",
    "mn_l = DayLocator()\n",
    "ax.xaxis.set_minor_locator(mn_l)\n",
    "mj_l = AutoDateLocator()\n",
    "mj_f = ConciseDateFormatter(mj_l, show_offset=False)\n",
    "ax.xaxis.set_major_formatter(mj_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test (predict) using OWID data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = \"India\"\n",
    "n_days = 200\n",
    "\n",
    "pop_fct = df.loc[df.location==c, 'population'].iloc[0] / 1000\n",
    "test_data = np.array(df.loc[(df.location==c) & (df.total_cases>=100), 'new_cases'].rolling(7, center=True, min_periods=1).mean() / pop_fct, dtype=np.float32)\n",
    "\n",
    "in_data = test_data[-IP_SEQ_LEN:]\n",
    "out_data = np.array([], dtype=np.float32)\n",
    "for i in range(int(n_days / OP_SEQ_LEN)):\n",
    "    ip = torch.tensor(\n",
    "        in_data,\n",
    "        dtype=torch.float32\n",
    "    )\n",
    "    ip = ip.to(device)\n",
    "    pred = model.predict(ip.view(1, IP_SEQ_LEN, 1))\n",
    "    in_data = np.append(in_data[-IP_SEQ_LEN+OP_SEQ_LEN:], pred.cpu().numpy())\n",
    "    out_data = np.append(out_data, pred.cpu().numpy())\n",
    "\n",
    "orig_df = pd.DataFrame({\n",
    "    'actual': test_data * pop_fct\n",
    "})\n",
    "fut_df = pd.DataFrame({\n",
    "    'predicted': out_data * pop_fct\n",
    "})\n",
    "# print(fut_df['predicted'].astype('int').to_csv(sep='|', index=False))\n",
    "orig_df = orig_df.append(fut_df, ignore_index=True, sort=False)\n",
    "orig_df['total'] = (orig_df['actual'].fillna(0) + orig_df['predicted'].fillna(0)).cumsum()\n",
    "\n",
    "start_date = df.loc[(df.location==c) & (df.total_cases>=100)]['date'].iloc[0]\n",
    "orig_df['Date'] = pd.Series([start_date + dt.timedelta(days=i) for i in range(len(orig_df))])\n",
    "ax = orig_df.plot(\n",
    "    x='Date',\n",
    "    y=['actual', 'predicted'],\n",
    "    title=c + ' daily cases',\n",
    "    figsize=(10,6),\n",
    "    grid=True\n",
    ")\n",
    "mn_l = DayLocator()\n",
    "ax.xaxis.set_minor_locator(mn_l)\n",
    "mj_l = AutoDateLocator()\n",
    "mj_f = ConciseDateFormatter(mj_l, show_offset=False)\n",
    "ax.xaxis.set_major_formatter(mj_f)\n",
    "# orig_df['total'] = orig_df['total'].astype('int')\n",
    "# orig_df['predicted'] = orig_df['predicted'].fillna(0).astype('int')\n",
    "# print(orig_df.tail(n_days))\n",
    "\n",
    "# arrow\n",
    "# peakx = 172\n",
    "# peak = orig_df.iloc[peakx]\n",
    "# peak_desc = peak['Date'].strftime(\"%d-%b\") + \"\\n\" + str(int(peak['predicted']))\n",
    "# _ = ax.annotate(\n",
    "#     peak_desc, \n",
    "#     xy=(peak['Date'] - dt.timedelta(days=1), peak['predicted']),\n",
    "#     xytext=(peak['Date'] - dt.timedelta(days=45), peak['predicted'] * .9),\n",
    "#     arrowprops={},\n",
    "#     bbox={'facecolor':'white'}\n",
    "# )\n",
    "\n",
    "# _ = ax.axvline(x=peak['Date'], linewidth=1, color='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statewise prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r=rq.get('https://api.covid19india.org/v3/min/timeseries.min.json')\n",
    "ts = r.json()\n",
    "\n",
    "data = []\n",
    "for state in ts:\n",
    "    for date in ts[state]:\n",
    "        data.append((state, date, ts[state][date]['total'].get('confirmed', 0)))\n",
    "\n",
    "states_df = pd.DataFrame(data, columns=['state', 'date', 'total'])\n",
    "states_df['date'] = pd.to_datetime(states_df['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://www.populationu.com/india-population\n",
    "STT_INFO = {\n",
    "    'AN' : {\"name\": \"Andaman & Nicobar Islands\", \"popn\": 450000},\n",
    "    'AP' : {\"name\": \"Andhra Pradesh\", \"popn\": 54000000},\n",
    "    'AR' : {\"name\": \"Arunachal Pradesh\", \"popn\": 30000000},\n",
    "    'AS' : {\"name\": \"Asaam\", \"popn\": 35000000},\n",
    "    'BR' : {\"name\": \"Bihar\", \"popn\": 123000000},\n",
    "    'CH' : {\"name\": \"Chandigarh\", \"popn\": 1200000},\n",
    "    'CT' : {\"name\": \"Chhattisgarh\", \"popn\": 29000000},\n",
    "    'DL' : {\"name\": \"Delhi\", \"popn\": 19500000},\n",
    "    'DN' : {\"name\": \"Dadra & Nagar Haveli and Daman & Diu\", \"popn\": 700000},\n",
    "    'GA' : {\"name\": \"Goa\", \"popn\": 1580000},\n",
    "    'GJ' : {\"name\": \"Gujarat\", \"popn\": 65000000},\n",
    "    'HP' : {\"name\": \"Himachal Pradesh\", \"popn\": 7400000},\n",
    "    'HR' : {\"name\": \"Haryana\", \"popn\": 28000000},\n",
    "    'JH' : {\"name\": \"Jharkhand\", \"popn\": 38000000},\n",
    "    'JK' : {\"name\": \"Jammu & Kashmir\", \"popn\": 13600000},\n",
    "    'KA' : {\"name\": \"Karnataka\", \"popn\": 67000000},\n",
    "    'KL' : {\"name\": \"Kerala\", \"popn\": 36000000},\n",
    "    'LA' : {\"name\": \"Ladakh\", \"popn\": 325000},\n",
    "    'MH' : {\"name\": \"Maharashtra\", \"popn\": 122000000},\n",
    "    'ML' : {\"name\": \"Meghalaya\", \"popn\": 3400000},\n",
    "    'MN' : {\"name\": \"Manipur\", \"popn\": 3000000},\n",
    "    'MP' : {\"name\": \"Madhya Pradesh\", \"popn\": 84000000},\n",
    "    'MZ' : {\"name\": \"Mizoram\", \"popn\": 1200000},\n",
    "    'NL' : {\"name\": \"Nagaland\", \"popn\": 2200000},\n",
    "    'OR' : {\"name\": \"Odisha\", \"popn\": 46000000},\n",
    "    'PB' : {\"name\": \"Punjab\", \"popn\": 30000000},\n",
    "    'PY' : {\"name\": \"Puducherry\", \"popn\": 1500000},\n",
    "    'RJ' : {\"name\": \"Rajasthan\", \"popn\": 80000000},\n",
    "    'TG' : {\"name\": \"Telangana\", \"popn\": 39000000},\n",
    "    'TN' : {\"name\": \"Tamil Nadu\", \"popn\": 77000000},\n",
    "    'TR' : {\"name\": \"Tripura\", \"popn\": 4100000},\n",
    "    'UP' : {\"name\": \"Uttar Pradesh\", \"popn\": 235000000},\n",
    "    'UT' : {\"name\": \"Uttarakhand\", \"popn\": 11000000},\n",
    "    'WB' : {\"name\": \"West Bengal\", \"popn\": 98000000},\n",
    "#     'SK' : {\"name\": \"Sikkim\", \"popn\": 681000},\n",
    "#     'UN' : {\"name\": \"Unassigned\", \"popn\": 40000000}, #avg pop\n",
    "#     'LD' : {\"name\": \"Lakshadweep\", \"popn\": 75000}\n",
    "}\n",
    "\n",
    "# uncomment for India\n",
    "# STT_INFO = {\n",
    "#     'TT' : {\"name\": \"India\", \"popn\": 1387155000}\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dummy state data: fruit country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy data for testing\n",
    "# SET 1 - 10 states\n",
    "# STT_INFO = {\n",
    "#     'A': {\"name\": \"Apple\", \"popn\": 10000000},\n",
    "#     'B': {\"name\": \"Berry\", \"popn\": 10000000},\n",
    "#     'C': {\"name\": \"Cherry\", \"popn\": 10000000},\n",
    "#     'D': {\"name\": \"Dates\", \"popn\": 10000000},\n",
    "#     'E': {\"name\": \"Elderberry\", \"popn\": 10000000},\n",
    "#     'F': {\"name\": \"Fig\", \"popn\": 10000000},\n",
    "#     'G': {\"name\": \"Grape\", \"popn\": 10000000},\n",
    "#     'H': {\"name\": \"Honeysuckle\", \"popn\": 10000000},\n",
    "#     'I': {\"name\": \"Icaco\", \"popn\": 10000000},\n",
    "#     'J': {\"name\": \"Jujube\", \"popn\": 10000000},\n",
    "# }\n",
    "# total = 100\n",
    "# SET 2 - 1 agg state\n",
    "STT_INFO = {\n",
    "    'Z': {\"name\": \"FruitCountry1000x\", \"popn\": 10000000},\n",
    "}\n",
    "total = 1000\n",
    "\n",
    "\n",
    "r = {\n",
    "    'state': [],\n",
    "    'date': [],\n",
    "    'total': []\n",
    "}\n",
    "\n",
    "start_date = dt.datetime(day=1, month=3, year=2020)\n",
    "end_date = dt.datetime.now()\n",
    "while start_date <= end_date:\n",
    "    for s in STT_INFO:\n",
    "        r['state'].append(s)\n",
    "        r['date'].append(start_date)\n",
    "        r['total'].append(total)\n",
    "    total *= 1.03\n",
    "    start_date += dt.timedelta(days=1)\n",
    "states_df = pd.DataFrame(r)\n",
    "states_df['date'] = pd.to_datetime(states_df['date'])\n",
    "states_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand(df):\n",
    "    '''Fill missing dates in an irregular timeline'''\n",
    "    min_date = df['date'].min()\n",
    "    max_date = df['date'].max()\n",
    "    idx = pd.date_range(min_date, max_date)\n",
    "    \n",
    "    df.index = pd.DatetimeIndex(df.date)\n",
    "    df = df.drop(columns=['date'])\n",
    "    return df.reindex(idx, method='pad').reset_index().rename(columns={'index':'date'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_days = 200 # number of days for prediction\n",
    "agg_days = n_days + 60 # number of days for plotting agg curve i.e. prediction + actual data \n",
    "\n",
    "states_agg = np.zeros(agg_days)\n",
    "ax = None\n",
    "api = {}\n",
    "for state in STT_INFO:\n",
    "    pop_fct = STT_INFO[state][\"popn\"] / 1000\n",
    "    \n",
    "    state_df = states_df.loc[states_df['state']==state][:-1] # skip todays data. covid19 returns incomplete.\n",
    "    state_df = expand(state_df)\n",
    "    state_df['daily'] = state_df['total'] - state_df['total'].shift(1).fillna(0)\n",
    "    test_data = np.array(state_df['daily'].rolling(7, center=True, min_periods=1).mean() / pop_fct, dtype=np.float32)\n",
    "\n",
    "    in_data = test_data[-IP_SEQ_LEN:]\n",
    "    out_data = np.array([], dtype=np.float32)\n",
    "    for i in range(int(n_days / OP_SEQ_LEN)):\n",
    "        ip = torch.tensor(\n",
    "            in_data,\n",
    "            dtype=torch.float32\n",
    "        ).to(device)\n",
    "        try:\n",
    "            pred = model.predict(ip.view(-1, IP_SEQ_LEN, 1))\n",
    "        except Exception as e:\n",
    "            print(state, e)\n",
    "        in_data = np.append(in_data[-IP_SEQ_LEN+OP_SEQ_LEN:], pred.cpu().numpy())\n",
    "        out_data = np.append(out_data, pred.cpu().numpy())\n",
    "    \n",
    "    sn = STT_INFO[state]['name']\n",
    "    orig_df = pd.DataFrame({\n",
    "        'actual': np.array(test_data * pop_fct, dtype=np.int)\n",
    "    })\n",
    "    fut_df = pd.DataFrame({\n",
    "        'predicted': np.array(out_data * pop_fct, dtype=np.int)\n",
    "    })\n",
    "    # print(fut_df.to_csv(sep='|'))\n",
    "    orig_df = orig_df.append(fut_df, ignore_index=True, sort=False)\n",
    "    orig_df[sn] = orig_df['actual'].fillna(0) + orig_df['predicted'].fillna(0)\n",
    "    orig_df['total'] = orig_df[sn].cumsum()\n",
    "    \n",
    "    states_agg += orig_df[sn][-agg_days:]\n",
    "\n",
    "    # generate date col for orig_df from state_df\n",
    "    start_date = state_df['date'].iloc[0]\n",
    "    orig_df['Date'] = pd.to_datetime([(start_date + dt.timedelta(days=i)).strftime(\"%Y-%m-%d\") for i in range(len(orig_df))])\n",
    "#     if orig_df[sn].max() < 10000: # or orig_df[sn].max() < 5000:\n",
    "#         continue\n",
    "    \n",
    "    # print state, cumulative, peak\n",
    "    peak = orig_df.loc[orig_df[sn].idxmax()]\n",
    "    print(sn, \"|\", peak['Date'].strftime(\"%b %d\"), \"|\", int(peak[sn]), \"|\", int(orig_df['total'].iloc[-1]))\n",
    "    \n",
    "    # export data for API\n",
    "    orig_df['deceased_daily'] = orig_df[sn] * 0.028\n",
    "    orig_df['recovered_daily'] = orig_df[sn].shift(14, fill_value=0) - orig_df['deceased_daily'].shift(7, fill_value=0)\n",
    "    orig_df['active_daily'] = orig_df[sn] - orig_df['recovered_daily'] - orig_df['deceased_daily']\n",
    "#     orig_df['deceased_total'] = orig_df['deceased_daily'].cumsum()\n",
    "#     orig_df['recovered_total'] = orig_df['recovered_daily'].cumsum()\n",
    "#     orig_df['active_total'] = orig_df['active_daily'].cumsum()\n",
    "    \n",
    "    api[state] = {}\n",
    "    for idx, row in orig_df[-n_days:].iterrows():\n",
    "        row_date = row['Date'].strftime(\"%Y-%m-%d\")\n",
    "        api[state][row_date] = {\n",
    "            \"delta\": {\n",
    "                \"confirmed\": int(row[sn]),\n",
    "                \"deceased\": int(row['deceased_daily']),\n",
    "                \"recovered\": int(row['recovered_daily']),\n",
    "                \"active\": int(row['active_daily'])\n",
    "            },\n",
    "#             \"total\": {\n",
    "#                 \"confirmed\": int(row['total']),\n",
    "#                 \"deceased\": int(row['deceased_total']),\n",
    "#                 \"recovered\": int(row['recovered_total']),\n",
    "#                 \"active\": int(row['active_total'])\n",
    "#             }\n",
    "        }\n",
    "        \n",
    "    # plot state chart\n",
    "    ax = orig_df.plot(\n",
    "        x='Date',\n",
    "        y=[sn],\n",
    "        title='Daily Cases',\n",
    "        figsize=(15,10),\n",
    "        grid=True,\n",
    "        ax=ax,\n",
    "        lw=3\n",
    "    )\n",
    "    mn_l = DayLocator()\n",
    "    ax.xaxis.set_minor_locator(mn_l)\n",
    "    mj_l = AutoDateLocator()\n",
    "    mj_f = ConciseDateFormatter(mj_l, show_offset=False)\n",
    "    ax.xaxis.set_major_formatter(mj_f)\n",
    "\n",
    "# plot aggregate chart\n",
    "cum_df = pd.DataFrame({\n",
    "    'states_agg': states_agg \n",
    "})\n",
    "last_date = orig_df['Date'].iloc[-1].to_pydatetime()\n",
    "start_date = last_date - dt.timedelta(days=agg_days)\n",
    "cum_df['Date'] = pd.to_datetime([(start_date + dt.timedelta(days=i)).strftime(\"%Y-%m-%d\") for i in range(len(cum_df))])\n",
    "ax = cum_df.plot(\n",
    "    x='Date',\n",
    "    y=['states_agg'],\n",
    "    title='Aggregate daily cases',\n",
    "    figsize=(15,10),\n",
    "    grid=True,\n",
    "    lw=3\n",
    ")\n",
    "mn_l = DayLocator()\n",
    "ax.xaxis.set_minor_locator(mn_l)\n",
    "mj_l = AutoDateLocator()\n",
    "mj_f = ConciseDateFormatter(mj_l, show_offset=False)\n",
    "ax.xaxis.set_major_formatter(mj_f)\n",
    "\n",
    "# plot peak in agg\n",
    "peakx = 178\n",
    "peak = cum_df.iloc[peakx]\n",
    "peak_desc = peak['Date'].strftime(\"%d-%b\") + \"\\n\" + str(int(peak['states_agg']))\n",
    "_ = ax.annotate(\n",
    "    peak_desc, \n",
    "    xy=(peak['Date'] + dt.timedelta(days=1), peak['states_agg']),\n",
    "    xytext=(peak['Date'] + dt.timedelta(days=45), peak['states_agg'] * .9),\n",
    "    arrowprops={},\n",
    "    bbox={'facecolor':'white'}\n",
    ")\n",
    "_ = ax.axvline(x=peak['Date'], linewidth=1, color='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export JSON for API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate predictions\n",
    "api['TT'] = {}\n",
    "for state in api:\n",
    "    if state == 'TT':\n",
    "        continue\n",
    "    for date in api[state]:\n",
    "        api['TT'][date] = api['TT'].get(date, {'delta':{}, 'total':{}})\n",
    "        for k in ['delta']: #'total'\n",
    "            api['TT'][date][k]['confirmed'] = api['TT'][date][k].get('confirmed', 0) + api[state][date][k]['confirmed']\n",
    "            api['TT'][date][k]['deceased'] = api['TT'][date][k].get('deceased', 0) + api[state][date][k]['deceased']\n",
    "            api['TT'][date][k]['recovered'] = api['TT'][date][k].get('recovered', 0) + api[state][date][k]['recovered']\n",
    "            api['TT'][date][k]['active'] = api['TT'][date][k].get('active', 0) + api[state][date][k]['active']\n",
    "\n",
    "# export\n",
    "with open(\"predictions.json\", \"w\") as f:\n",
    "    f.write(json.dumps(api, sort_keys=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
